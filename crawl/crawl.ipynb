{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Khởi tạo trình duyệt Chrome\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Truy cập trang web\n",
    "driver.get(\"https://i-batdongsan.com/can-ban-nha-mat-tien.htm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy link các page từ trang 1 đến trang 1000\n",
    "link_page = []\n",
    "for i in range(1,271):\n",
    "    link_page.append('https://i-batdongsan.com/can-ban-nha-dat/p' + str(i) +'.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truy cập vào từng page và lấy link các bài đăng\n",
    "link_post = []\n",
    "for i in link_page:\n",
    "    driver.get(i)\n",
    "    sleep(1.5)\n",
    "    link = driver.find_elements(By.XPATH,'//*[@class=\"ct_title\"]/a')\n",
    "    for j in link:\n",
    "        link_post.append(j.get_attribute('href'))\n",
    "        # viết link vào file txt theo dạng danh sách vào F:\\Thống Kê Máy Tính\\Crawl_Data\\file_txt\\link_post.txt\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\link_post1.txt','w') as f:\n",
    "            for item in link_post:\n",
    "                f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dùng bs4 để lấy thông tin từng bài đăng\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "list_dia_chi= []\n",
    "list_price = []\n",
    "list_infor = []\n",
    "list_ten_LH = []\n",
    "list_sdt_LH = []\n",
    "# lấy thông tin từng bài đăng thông qua link đã lấy được\n",
    "# truy cập vào từng link nếu link nào bị lỗi thì bỏ qua link đó, đọc file txt để lấy link\n",
    "with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\link_post1.txt', 'r') as f:\n",
    "    link_post = f.read().splitlines()\n",
    "for i in tqdm(link_post):\n",
    "    try:\n",
    "        response = requests.get(i)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # lấy giá\n",
    "        price = soup.find('td', class_ = 'price').text.strip()\n",
    "        list_price.append(price)\n",
    "\n",
    "        # viết giá vào file txt theo dạng danh sách\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_price1.txt', 'w', encoding='utf-8') as file:\n",
    "            for chuoi in list_price:\n",
    "                file.write(chuoi + '\\n')\n",
    "\n",
    "        # lấy địa chỉ\n",
    "        dia_chi = soup.find('span', class_ = 'value').text\n",
    "        list_dia_chi.append(dia_chi)\n",
    "\n",
    "        # viết địa chỉ vào file txt theo dạng danh sách\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_dia_chi1.txt', 'w', encoding='utf-8') as file:\n",
    "            for chuoi in list_dia_chi:\n",
    "                file.write(chuoi + '\\n')\n",
    "\n",
    "        # lấy thông tin liên hệ\n",
    "        LH = soup.find('div', class_ = 'name').text.strip()\n",
    "        list_ten_LH.append(LH)\n",
    "\n",
    "        # # viết thông tin liên hệ vào file txt theo dạng danh sách\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_ten_LH1.txt', 'w', encoding='utf-8') as file:\n",
    "            for chuoi in list_ten_LH:\n",
    "                file.write(chuoi + '\\n')\n",
    "\n",
    "        # lấy số điện thoại liên hệ\n",
    "        sdt_LH = soup.find('div', class_ = 'fone').text.strip()\n",
    "        list_sdt_LH.append(sdt_LH)\n",
    "\n",
    "        # # viết số điện thoại liên hệ vào file txt theo dạng danh sách\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_sdt_LH1.txt', 'w', encoding='utf-8') as file:\n",
    "            for chuoi in list_sdt_LH:\n",
    "                file.write(chuoi + '\\n')\n",
    "\n",
    "        # lấy thông tin chi tiết\n",
    "        infor = soup.find('div', class_ = 'infor')\n",
    "        # tách thông tin chi tiết thành từng phần vì có nhiều 'tr' trong 'div' infor\n",
    "        infor = infor.find_all('td')\n",
    "        # lấy thông tin chi tiết\n",
    "        infor = [i.text.strip() for i in infor]\n",
    "        list_infor.append(infor)\n",
    "\n",
    "        # # viết thông tin chi tiết vào file txt theo dạng danh sách\n",
    "        with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_infor1.txt', 'w', encoding='utf-8') as file:\n",
    "            for sublist in list_infor:\n",
    "                # Sử dụng join để nối các phần tử trong sublist thành một chuỗi và ghi vào tệp\n",
    "                line = '\\t'.join(sublist)  # Sử dụng ký tự tab để phân tách các phần tử trong sublist\n",
    "                file.write(line + '\\n')\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(list_price))\n",
    "print(len(list_ten_LH))\n",
    "print(len(list_sdt_LH))\n",
    "print(len(list_infor))\n",
    "print(len(list_dia_chi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đọc file txt để lấy thông tin\n",
    "with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_price1.txt', 'r', encoding='utf-8') as f:\n",
    "    list_price = f.read().splitlines()\n",
    "with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_ten_LH1.txt', 'r', encoding='utf-8') as f:\n",
    "    list_ten_LH = f.read().splitlines()\n",
    "with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_sdt_LH1.txt', 'r', encoding='utf-8') as f:\n",
    "    list_sdt_LH = f.read().splitlines()\n",
    "with open('F:\\Thống Kê Máy Tính\\Crawl_Data/file_txt\\list_infor1.txt', 'r', encoding='utf-8') as f:\n",
    "    list_infor = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_price)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
